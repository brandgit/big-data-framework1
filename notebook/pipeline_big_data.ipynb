{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification des version et test de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Version de Python :\", sys.version)\n",
    "\n",
    "import pyspark\n",
    "print(\"Version de PySpark :\", pyspark.__version__)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Version de Spark :\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une session de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestSpark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction du dataset et sauvegarde dans HDFS au format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Parquet\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "csv_file_path = \"/home/jovyan/work/datasets/train.csv\"  \n",
    "\n",
    "\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "print(\"Noms des colonnes:\", df.columns)\n",
    "\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "\n",
    "parquet_output_path = \"hdfs://namenode:8020/user/jovyan/bronze/train.parquet\"\n",
    "\n",
    "\n",
    "df.write.parquet(parquet_output_path)\n",
    "print(\"Sauvegade du dataset avec succès dans hdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du dataset brut depuis HDFS pour un cleanning et stocker la data nettoyé dans une table Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim, col\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Parquet Cleaning\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "parquet_file_path = \"hdfs://namenode:9000/user/jovyan/bronze/train.parquet\"\n",
    "\n",
    "\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "print(\"Chargement du fichier avec succès depuis hdfs\")\n",
    "\n",
    "# Nettoyage des données\n",
    "\n",
    "# 1. Suppression des espaces dans les colonnes de type string\n",
    "string_columns = [col_name for col_name, dtype in df.dtypes if dtype == \"string\"]\n",
    "df_cleaned = df.select([trim(col(c)).alias(c) if c in string_columns else col(c) for c in df.columns])\n",
    "\n",
    "# 2. Gestion des valeurs manquantes\n",
    "df_cleaned = df_cleaned.na.fill(0)\n",
    "\n",
    "# 3. Convertion des Colonnes en int\n",
    "int_columns = ['ID', 'Customer_ID', 'Age', 'SSN', 'Num_Bank_Accounts', 'Num_Credit_Card', \n",
    "               'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', \n",
    "               'Num_Credit_Inquiries']\n",
    "df_cleaned = df_cleaned.select([col(c).cast(\"int\").alias(c) if c in int_columns else col(c) for c in df_cleaned.columns])\n",
    "\n",
    "# Convertion des Colonnes  en float\n",
    "float_columns = ['Annual_Income', 'Monthly_Inhand_Salary', 'Changed_Credit_Limit', 'Outstanding_Debt', \n",
    "                 'Credit_Utilization_Ratio', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Monthly_Balance', \n",
    "                 'Credit_History_Age']\n",
    "df_cleaned = df_cleaned.select([col(c).cast(\"float\").alias(c) if c in float_columns else col(c) for c in df_cleaned.columns])\n",
    "\n",
    "# Suppression des doublons\n",
    "df_cleaned = df_cleaned.dropDuplicates(subset=['ID', 'Customer_ID', 'Name'])\n",
    "\n",
    "# Catégorisation de l'âge\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"Age_Group\", \n",
    "    when(df_cleaned.Age < 25, \"Young\").when((df_cleaned.Age >= 25) & (df_cleaned.Age <= 35), \"Adult\").when((df_cleaned.Age > 35) & (df_cleaned.Age <= 50), \"Middle-Aged\").otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "\n",
    "df_cleaned.printSchema()\n",
    "\n",
    "df_cleaned.show(5)\n",
    "\n",
    "cleaned_parquet_output_path = \"hdfs://namenode:8020/user/jovyan/silver/cleaned_train.parquet\"\n",
    "\n",
    "# Sauvegarde du DataFrame nettoyé en format Parquet dans HDFS\n",
    "df_cleaned.write.mode(\"overwrite\").parquet(cleaned_parquet_output_path)\n",
    "\n",
    "print(f\"Dataset nettoyé sauvegardé avec succès dans {cleaned_parquet_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sauvegarde du DataFrame nettoyé dans une table Hive\n",
    "hive_table_name = \"cleaned_train_data\"\n",
    "\n",
    "df_cleaned.write.mode(\"overwrite\").saveAsTable(hive_table_name)\n",
    "\n",
    "print(f\"Dataset nettoyé sauvegardé avec succès dans la table Hive : {hive_table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitionnement de notre Datawarehouse en DataMarts dans une base de donnée MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sqlalchemy import create_engine\n",
    "!pip install mysql-connector-python\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Datamart Cleaning and Storage\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "csv_file_path = \"/home/jovyan/work/datasets/cleaned_dataset.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Datamart des Profils Clients\n",
    "datamart_clients = df.select(\n",
    "    'Customer_ID', 'Name', 'Age', 'SSN', 'Occupation', 'Annual_Income', 'Monthly_Inhand_Salary'\n",
    ")\n",
    "\n",
    "# Datamart des Comptes et Utilisation du Crédit\n",
    "datamart_credit = df.select(\n",
    "    'Customer_ID', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', \n",
    "    'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', \n",
    "    'Credit_Utilization_Ratio', 'Outstanding_Debt'\n",
    ")\n",
    "\n",
    "# Datamart du Risque de Crédit\n",
    "datamart_risk = df.select(\n",
    "    'Customer_ID', 'Credit_Mix', 'Payment_of_Min_Amount', 'Total_EMI_per_month', \n",
    "    'Payment_Behaviour', 'Credit_Score', 'Credit_History_Age'\n",
    ")\n",
    "\n",
    "# Datamart des Transactions et Investissements\n",
    "datamart_transactions = df.select(\n",
    "    'Customer_ID', 'Month', 'Amount_invested_monthly', 'Monthly_Balance', \n",
    "    'Outstanding_Debt', 'Payment_Behaviour'\n",
    ")\n",
    "\n",
    "df_clients_pd = datamart_clients.toPandas()\n",
    "df_credit_pd = datamart_credit.toPandas()\n",
    "df_risk_pd = datamart_risk.toPandas()\n",
    "df_transactions_pd = datamart_transactions.toPandas()\n",
    "\n",
    "# Connexion à la base de données MySQL avec les informations de notre config Docker\n",
    "engine = create_engine('mysql+mysqlconnector://root:rootpassword@mysql:3306/hive')\n",
    "\n",
    "# Sauvegarde des datamarts dans la base de données MySQL\n",
    "df_clients_pd.to_sql(name='datamart_clients', con=engine, if_exists='replace', index=False)\n",
    "df_credit_pd.to_sql(name='datamart_credit', con=engine, if_exists='replace', index=False)\n",
    "df_risk_pd.to_sql(name='datamart_risk', con=engine, if_exists='replace', index=False)\n",
    "df_transactions_pd.to_sql(name='datamart_transactions', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Datamarts sauvegardés avec succès dans la base de données MySQL\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
