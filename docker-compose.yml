version: '3.8'

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "8020:8020"
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - namenode_data:/hadoop/dfs/namenode
    networks:
      - hadoop-net

  datanode-1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - datanode-1_data:/hadoop/dfs/datanode
    ports:
      - "9870:9870"
    networks:
      - hadoop-net
    depends_on:
      - namenode

  datanode-2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    volumes:
      - datanode-2_data:/hadoop/dfs/datanode
    networks:
      - hadoop-net
    depends_on:
      - namenode

  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
    networks:
      - hadoop-net
    depends_on:
      - datanode-1
      - datanode-2

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - hadoop-net
    depends_on:
      - spark-master

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - hadoop-net
    depends_on:
      - spark-master

  jupyter:
    image: jupyter/pyspark-notebook
    container_name: jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./datasets:/home/jovyan/work/datasets
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - hadoop-net
    # depends_on:
    #   - spark-master

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    environment:
      - HIVE_METASTORE_HOST=hive-metastore
      - HIVE_DB=mysql
      - HIVE_USER=root
      - HIVE_PASSWORD=rootpassword
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020 
    ports:
      - "9083:9083"
    networks:
      - hadoop-net
    depends_on:
      - namenode
      - datanode-1
      - datanode-2
      - mysql

  hive-server:
    image: bde2020/hive:2.3.2
    container_name: hive-server
    environment:
      - SERVICE_PRECONDITION="hive-metastore:9083"
      - HIVE_DB=mysql
      - HIVE_USER=root
      - HIVE_PASSWORD=rootpassword
      - HIVE_METASTORE_URIS=thrift://hive-metastore:9083  
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020  
      - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL:jdbc:postgresql://hive-metastore/metastore
    ports:
      - "10000:10000"
    networks:
      - hadoop-net
    depends_on:
      - hive-metastore


  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: hive
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - hadoop-net
    
  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"  
    environment:
      - MB_DB_FILE=/metabase.db  
    volumes:
      - metabase-data:/metabase.db
    networks:
      - hadoop-net


volumes:
  namenode_data:
  datanode-1_data:
  datanode-2_data:
  mysql_data:
  hadoop-data:
  pgdata:
  hivemetastore:
  metabase-data:

networks:
  hadoop-net:
    driver: bridge
